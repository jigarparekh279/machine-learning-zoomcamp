{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for deploying ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same model we trained and evaluated\n",
    "previously - the churn prediction model. Now we'll\n",
    "deploy it as a web service.\n",
    "\n",
    "**SUMMARY**\n",
    "\n",
    "* Save models with pickle\n",
    "* Use `Flask` to turn the model into a web service (other: FastAPI)\n",
    "* Use `pipenv` dependency & env manager (other: virtual env, conda, poetry)\n",
    "* Package it in Docker \n",
    "* Deploy to the `AWS` cloud (other: GCP, Azure, Heroku, Python Anywhere) \n",
    "\n",
    "\n",
    "_Saving and loading the model_\n",
    "\n",
    "* Saving the model to pickle\n",
    "* Loading the model from pickle\n",
    "* Turning our notebook into a Python script\n",
    "\n",
    "_Web services: introduction to Flask_\n",
    "\n",
    "* Writing a simple ping/pong app\n",
    "* Querying it with `curl` and browser\n",
    "\n",
    "_Serving the churn model with Flask_\n",
    "\n",
    "* Wrapping the predict script into a Flask app\n",
    "* Querying it with `requests` \n",
    "* Preparing for production: gunicorn\n",
    "* Running it on Windows with waitress\n",
    "\n",
    "_Python virtual environment: Pipenv_\n",
    "\n",
    "* Dependency and environment management\n",
    "* Why we need virtual environment\n",
    "* Installing Pipenv\n",
    "* Installing libraries with Pipenv\n",
    "* Running things with Pipenv\n",
    "\n",
    "_Environment management: Docker_\n",
    "\n",
    "* Why we need Docker\n",
    "* Running a Python image with docker\n",
    "* Dockerfile\n",
    "* Building a docker image\n",
    "* Running a docker image\n",
    "\n",
    "_Deployment to the cloud: AWS Elastic Beanstalk_\n",
    "\n",
    "* Installing the eb cli\n",
    "* Running eb locally\n",
    "* Deploying the model\n",
    "\n",
    "**Explore more**\n",
    "\n",
    "* Flask is not the only framework for creating web services. Try others, e.g. FastAPI\n",
    "* Experiment with other ways of managing environment, e.g. virtual env, conda, poetry.\n",
    "* Explore other ways of deploying web services, e.g. GCP, Azure, Heroku, Python Anywhere, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most IMP Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment (Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Cross-industry standard process for data mining (CRISP-DM)](https://youtu.be/dCa3JvmJbr0?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR) as simple data science pipeline\n",
    "- [ROC AUC](https://youtu.be/hvIQPAwkVZo?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&t=419) probabilistic interpretation\n",
    "- [K-fold model validation](https://youtu.be/BIIZaVtUbf4?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&t=82) with model AUC $\\pm$ std dev\n",
    "- [Intro to model deployement](https://youtu.be/agIFak9A3m8?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "\n",
    "    <img src=\"2025-08-15_22-41.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "- [Web services and intro to Flask](https://youtu.be/W7ubna1Rfv8?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Serving the Churn Model with Flask](https://youtu.be/Q7ZWPgPnRz8?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "    \n",
    "    Some useful commands\n",
    "    ```bash\n",
    "    python predict.py\n",
    "    python predict-test.py # w/ dev server warning\n",
    "    gunicorn --bind 0.0.0.0:9696 predict:app # w/o warning \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Python Virtual Environment: Pipenv](https://youtu.be/BMXh8JGROHM?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "    --> for handling python dependencies on same machine\n",
    "\n",
    "    <img src=\"2025-08-16_18-29.png\" alt=\"alt text\" width=\"400\"/>\n",
    "    <img src=\"2025-08-16_18-31_1.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "    What are dev packages? A: Packages which we only need during the development phase and not in the production (deployment)\n",
    "\n",
    "    Different virtual envs creation libs\n",
    "\n",
    "    - virtual env (venv)\n",
    "    - conda\n",
    "    - **pipenv** (we use this)\n",
    "    - poetry\n",
    "    - ...\n",
    "\n",
    "    Some useful commands (TIP: do not run the following from an active env)\n",
    "    ```bash\n",
    "    pip install pipenv\n",
    "    pipenv install numpy pandas scikit-learn ... # create Pipfile and Pipfile.lock\n",
    "    pipenv install # install env\n",
    "    pipenv shell # activate env\n",
    "    ```\n",
    "\n",
    "    After activating the env make the inference using \n",
    "    \n",
    "    `run gunicorn --bind 0.0.0.0:9696 predict:app`. \n",
    "    \n",
    "    Activate the env only to make the inference in one shot using\n",
    "\n",
    "    `pipenv run gunicorn --bind=0.0.0.0:9696 predict:app`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Environment Management: Docker](https://youtu.be/wAtyYZ6zvAs?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "\n",
    "    <img src=\"2025-08-18_12-24.png\" alt=\"alt text\" width=\"400\"/>\n",
    "    <img src=\"2025-08-18_12-25.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "    Create a `Dockerfile` with these steps\n",
    "\n",
    "    NOTE: Get a docker base image from - https://hub.docker.com/_/python e.g. `3.9.23-slim`\n",
    "\n",
    "    **Dockerfile**\n",
    "\n",
    "    ```docker\n",
    "    # base image\n",
    "    FROM python:3.10-slim\n",
    "\n",
    "    # we need pipenv to install libs and create env\n",
    "    RUN pip install pipenv\n",
    "\n",
    "    # create and enter app dir\n",
    "    WORKDIR /app\n",
    "\n",
    "    # copy pipfiles in current dir (i.e. app dir)\n",
    "    COPY [\"Pipfile\", \"Pipfile.lock\", \"./\"]\n",
    "\n",
    "    # install all libs system wide as we don't want to create a\n",
    "    # virtual env in a docker image as it is already isolated\n",
    "    RUN pipenv install --system --deploy\n",
    "\n",
    "    # copy model and predict files\n",
    "    COPY [\"model_C=1.0.bin\", \"predict.py\", \"./\"]\n",
    "\n",
    "    # exposing port for communication\n",
    "    EXPOSE 9696\n",
    "\n",
    "    # entry point for directly running the request\n",
    "    ENTRYPOINT [\"gunicorn\",  \"--bind 0.0.0.0:9696\", \"predict:app\"]\n",
    "    ```\n",
    "    \n",
    "    **Build a docker image**\n",
    "    ```\n",
    "    docker build -t zoomcamp-test .\n",
    "    ```\n",
    "\n",
    "    **Run a docker image in terminal**\n",
    "\n",
    "    ```\n",
    "    docker run -it --rm zoomcamp-test:latest # using default or dockerfile entrypoint     \n",
    "    docker run -it --rm --entrypoint=bash zoomcamp-test:latest # explicitly asking for an entrypoint\n",
    "    ```\n",
    "\n",
    "    **PORT provides external access to the container**\n",
    "\n",
    "    <img src=\"2025-08-18_17-56.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "    Now expose the port on container to host machine using \n",
    "    \n",
    "    `-p <container_port>:<host_machine_port>`\n",
    "\n",
    "    This allows to use `predict-test.py` on host machine which communicates with the container\n",
    "    \n",
    "    ```\n",
    "    docker run -it --rm -p 9696:9696 zoomcamp-test-1:latest\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Deployment To The Cloud: AWS Elastic Beanstalk](https://youtu.be/HGPJ4ekhcLg?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "\n",
    "    <img src=\"2025-08-18_22-44.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "    _Install_ AWS Elastic Beanstalk `awsebcli` only as a _dev_ package to deploy our web service (not something we need to have inside the container). See line in Pipfile `[dev-packages]` and line `\"develop\": { ...` in Pipfile.lock.\n",
    "    ```\n",
    "    pipenv install awsebcli --dev\n",
    "    ```\n",
    "    \n",
    "    Then _get_ into the env (`pipenv shell`) and init a `eb` env on cloud\n",
    "    ```\n",
    "    pipenv shell\n",
    "    eb init -p docker -r eu-central-1 churn-serving\n",
    "    ```\n",
    "    Here `-p` is for platform which is docker and `-r` for region\n",
    "\n",
    "    _Create_ and instance of this cloud (AWS) env\n",
    "    ```\n",
    "    eb create churn-service-env\n",
    "    ```\n",
    "\n",
    "    _Add host_ and change the url in `predict-test.py` to:\n",
    "    ```python\n",
    "    host = \"churn-serving-env.eba-umxcsddh.eu-central-1.elasticbeanstalk.com\"\n",
    "    url = f'http://{host}/predict'\n",
    "    ```\n",
    "\n",
    "    As of now this EB env is public and anyone (any service can have access)\n",
    "\n",
    "    _Terminate_ of AWS EB machine\n",
    "    ```\n",
    "    eb terminate churn-serving-env\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serverless (Chapter 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-08-26_15-28.png\" alt=\"alt text\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**AWS Lambda**](https://youtu.be/_UX8-2WhHZo?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "\n",
    "- No need of any AWS EC2 machine/server but directly use Lambda function\n",
    "    \n",
    "    <img src=\"2025-08-26_15-49.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "- You pay per request\n",
    "    \n",
    "    <img src=\"2025-08-26_15-51.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "\n",
    "- Why use TF-lite?\n",
    "    - (Earlier) AWS Lambda limits ($\\le$ 50 MB zip file). But now upto 10 GB using containiers\n",
    "    - Large container image\n",
    "        - pay more: for storing the image !\n",
    "        - slow init: takes time to initialize the lambda function when we invoke it for the first time !\n",
    "        - slow to import: tf takes long to be imported in our python script !\n",
    "\n",
    "    - TF-lite only focuses on inference (not used for training models): `pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we follow these steps\n",
    "\n",
    "- Save (or download) the model\n",
    "\n",
    "- Create a python file for the model prediction and lambda handler -- call it e.g. `lambda_function.py`\n",
    "\n",
    "- Create a Docker image \n",
    "    - using a base image from AWS ECR (https://gallery.ecr.aws/) - look for `python lambda`\n",
    "    - `docker build -t clothing-model .` here `.` means that use the Docker image in this dir\n",
    "    - try it our locally first\n",
    "        - `docker run -it --rm -p 8080:8080 clothing-model:latest`\n",
    "        - `python test.py`\n",
    "\n",
    "- Publish the docker image on Amazon (Elastic Container Registry) ECR to be loaded later in AWS lambda\n",
    "    - (if not already done) Install and configure AWS Command Line Interface (awscli)\n",
    "        - `pip install awscli`\n",
    "        - `aws configure`\n",
    "    - create a repository for container images using AWS CLI\n",
    "        - `aws ecr create-repository --repository-name cloth ing-tflite-images`\n",
    "    - log into our container repository (or registry) as it is private using\n",
    "        - `aws ecr get-login-password --region eu-central-1 | docker login --username AWS --password-stdin 364155067933.dkr.ecr.eu-central-1.amazonaws.com` \n",
    "    - make a **URI** for the image that we are going to push to our ECR _cloth ing-tflite-images_\n",
    "        ```bash\n",
    "        ACCOUNT=364155067933\n",
    "        REGION=eu-central-1\n",
    "        REGISTRY=dogs-vs-cats-tflite-images\n",
    "        PREFIX=${ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/${REGISTRY}\n",
    "        TAG=dogs-vs-cats-model-xception-v1-001\n",
    "        REMOTE_URI=${PREFIX}:${TAG}\n",
    "        ```\n",
    "    - now **tag** and **push** (just like git commit and push) the _clothing-model:latest_ docker image previously created with the _URI_ we just created\n",
    "        - `docker tag clothing-model:latest ${REMOTE_URI}`\n",
    "        - `docker push ${REMOTE_URI}`       \n",
    "    - now create a AWS lambda function and use the (browse to) `clothing-model-xception-v4-001` image from  `clothing-tflite-images` repository\n",
    "    - test the model on AWS lambda (just like we tested it locally)\n",
    "\n",
    "- [Exposing the Lambda Function](https://youtu.be/wyZ9aqQOXvs?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR) as a web service using API Gateway\n",
    "\n",
    "    _API Gateway helps to expose different AWS services as web services including lambda functions_\n",
    "    - go to AWS and create an API - choose FAST API and create a resource e.g. `predict` with a method `POST` and a stage e.g. `test`\n",
    "    - deploy (expose) this API - which creates a ULR for testing our gateway (web service) from anywhere\n",
    "    - we can than use this AWS lambda gateway URL (_see invoke URL in method tab_) in `test.py` instead of our local machine URL to directly use our web service!\n",
    "    - **NOTE**: this URL is by default public and anyone can send requests just like we do in `test.py`. So for work we should make it private or limited to people who should have access to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [TF-Serving | Kubernetes (Chapter 10)](https://youtu.be/mvPER7YfTkw?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2025-08-28_15-56.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "_Note that we would make this service very efficient by splitting it into 2 services -- as we can use CPUs for gateway service and GPUs for TF-serving service separately!_\n",
    "\n",
    "We move on to more than once services (with their respective Docker images) - use `docker-compose` and `kubrnetes` for communication between them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Serving\n",
    "\n",
    "#######\n",
    "* The saved_model format \n",
    "* Running TF-Serving locally with Docker\n",
    "* Invoking the model from Jupyter\n",
    "\n",
    "#######\n",
    "\n",
    "- To use TF-Serving we need to first convert tf model to a special format called _saved model_ (similar to what we did earlier using _tflite_).\n",
    "\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "\n",
    "    model = keras.models.load_model(\"clothing-model-v4.h5\")\n",
    "    tf.saved_model.save(model, \"clothing-model\")\n",
    "    ```\n",
    "\n",
    "- We can now look inside this model using `saved_model_cli` which comes already with a tensorflow download.\n",
    "\n",
    "    ```bash\n",
    "    saved_model_cli show --dir clothing-model --all\n",
    "    ```\n",
    "    Look for signature, inputs and outputs in _signature-def['serving_default']_ and store in the names of the input and output (e.g. in model-description.txt)\n",
    "\n",
    "- Now we can run tf-serving using this model \n",
    "    - Instead of creating our docker image (as before) we first spin up an official tf-serving docker image (locally) and mount our model on it \n",
    "\n",
    "        ```bash\n",
    "        docker run -it --rm \\                                                  \n",
    "            -p 8500:8500 \\\n",
    "            -v \"$(pwd)/clothing-model:/models/clothing-model/1\" \\\n",
    "            -e MODEL_NAME=\"clothing-model\" \\\n",
    "            tensorflow/serving:2.7.0\n",
    "        ```\n",
    "\n",
    "    Here -p if for port mapping HOST:REMOTE (container), -v for volume mounting for the model from HOST:REMOTE, and last line is image name\n",
    "\n",
    "    - We now need install `grpc` client which is a special protocol which uses binary data format (faster than json) used to communicate with tf-serving\n",
    "        ```\n",
    "        pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n",
    "        ```\n",
    "\n",
    "    - We then create the code that would go (as usual) in our inference script (like predict.py or lambda_function.py)\n",
    "\n",
    "        ```python\n",
    "        import grpc\n",
    "        from tensorflow_serving.apis import predict_pb2\n",
    "        from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "        host = 'localhost:8500'# port where our tf-service is currently running (see above docker run)\n",
    "\n",
    "        # access this port (using insecure as tf-service will not be accessible from outside of Kubernetes)\n",
    "        channel = grpc.insecure_channel(host)\n",
    "        # tf-serving\n",
    "        stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "        from keras_image_helper import create_preprocessor\n",
    "        preprocessor = create_preprocessor('xception', target_size=(299, 299))\n",
    "        url = 'http://bit.ly/mlbookcamp-pants'\n",
    "        X = preprocessor.from_url(url)  \n",
    "        # want to send this X to our prediction service which is currently running in tf-serving\n",
    "\n",
    "        # now we want to make a proto buf request and make the prediction using the tf-serving which is currently running \n",
    "        def np_to_protobuf(data):\n",
    "            return tf.make_tensor_proto(data, shape=data.shape)\n",
    "        pb_request = predict_pb2.PredictRequest()\n",
    "        pb_request.model_spec.name = 'clothing-model'\n",
    "        pb_request.model_spec.signature_name = 'serving_default'\n",
    "        pb_request.inputs['input_8'].CopyFrom(np_to_protobuf(X))\n",
    "        pb_response = stub.Predict(pb_request, timeout=20.0)\n",
    "        preds = pb_response.outputs['dense_7'].float_val\n",
    "        ```\n",
    "        As usual we turn the output into a dictionary as before.\n",
    "\n",
    "    - So in summary \n",
    "        - this is how we communicate with a model deployed with tensorflow-serving\n",
    "        - we build the request (a step we always do to create a service) containing - name of the model, signature and the input\n",
    "        - we then prepare and send this proto buf request which we send over grpc to our instance of tf-serving where the model processes it and gives the predictions back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing service\n",
    "\n",
    "#######\n",
    "\n",
    "* Converting the notebook to a Python script\n",
    "* Wrapping the script into a Flask app\n",
    "* Creating the virtual env with Pipenv\n",
    "* Getting rid of the tensorflow dependency\n",
    "\n",
    "#######\n",
    "\n",
    "- We have every thing in the above image except for a flask service. Lets call this service as gateway.py (as in the image) as opposed to previously called predict.py or lambda_function.py\n",
    "\n",
    "    - convert the above code into functions - `prepare_request(X), prepare_response(pb_response), predict(url),` and add a \\_\\_main__ function:\n",
    "        ```python\n",
    "        if __name__ == '__main__':\n",
    "        url = 'http://bit.ly/mlbookcamp-pants'\n",
    "        response = predict(url)\n",
    "        print(response)\n",
    "        ```\n",
    "    - test by executing the script (note that the docker still needs to be running): `python gateway.py`\n",
    "\n",
    "    - now we turn this script into a Flask application (see chapter 5) by adding:\n",
    "        ```python\n",
    "        app = Flask('gateway')  # 'gateway' is the name of the app (service) -  can be anything\n",
    "\n",
    "        @app.route('/predict', methods=['POST'])\n",
    "        def predict_endpoint():\n",
    "            data = request.get_json()\n",
    "            url = data['url']\n",
    "            result = predict(url)\n",
    "            return jsonify(result)\n",
    "\n",
    "        if __name__ == '__main__':\n",
    "            app.run(debug=True, host='0.0.0.0', port=9696)\n",
    "\n",
    "        ```\n",
    "\n",
    "        Now running this script will invoke the Flask app to which we can post requests using a test.py script (as we did in chapter 9). \n",
    "        \n",
    "        NOTE that this test only works when the docker image is still running and the flas app is still invoked! This allows us to send the `REQUEST` to the `GATEWAY` which then `PREPARES` the request and sends it to `TF_SERVING` which uses the `MODEL` to make the `PREDICTION` and sends it back to `GATEWAY` which then post-processes and sends it to us!\n",
    "\n",
    "- Creating a pipenv env\n",
    "\n",
    "    ```\n",
    "    pipenv install grpcio==1.42.0 flask gunicorn keras-image-helper\n",
    "    ```\n",
    "\n",
    "- Remove dependence on tensorflow in gateway\n",
    "\n",
    "    We have this code which depends on tf\n",
    "    ```python\n",
    "    def np_to_protobuf(data):\n",
    "    return tf.make_tensor_proto(data, shape=data.shape)\n",
    "    ```\n",
    "\n",
    "    To avoid this we install `tensorflow-protobuf` (as tf cpu is also large)\n",
    "    ```\n",
    "    pipenv install tensorflow-protobuf\n",
    "    ```\n",
    "\n",
    "    And then use it (without tf) to change the function `np_to_protobuf` -- see proto.py. Test it by running the gateway (not using flask service) in pipenv shell (while the container running).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running everything locally with Docker-compose\n",
    "* Preparing the images \n",
    "* Installing docker-compose \n",
    "* Running the service \n",
    "* Testing the service\n",
    "\n",
    "- Previously we used an official tf-serving docker image and copied our model in it. Now we want to make a self contained docker image so that when we deploy it it has everything\n",
    "\n",
    "    - we first create (build and run) a docker file for tf-serving service see `image-model.dockerfile`\n",
    "\n",
    "        ```\n",
    "        docker build -t zoomcamp-10-model:xception-v4-001 -f image-model.dockerfile .\n",
    "        docker run -it --rm -p 8500:8500 zoomcamp-10-model:xception-v4-001\n",
    "        ```\n",
    "        \n",
    "        As before test it by running gateway (w/o flask) in pipenv shell\n",
    "\n",
    "    - then we create a docker file for gateway (flask service) -- see `image-gateway.dockerfile` [same as session 5]\n",
    "\n",
    "        ```\n",
    "        docker build -t zoomcamp-10-gateway:001 -f image-gateway.dockerfile .\n",
    "        docker run -it --rm -p 9696:9696 zoomcamp-10-gateway:001\n",
    "        ```\n",
    "\n",
    "    - to test this services together -- if we run test.py it runs into an error as out gateway is not able to communicate with the tensorflow service!\n",
    "\n",
    "        <img src=\"2025-09-16_13-11.png\" alt=\"alt text\" width=\"400\"/>\n",
    "\n",
    "\n",
    "- We now want to establish a communication between the two services -- precisely put them on same network for the ports to match\n",
    "\n",
    "    - we use docker compose to do this\n",
    "\n",
    "    ```\n",
    "    sudo apt install -y docker-compose-plugin \n",
    "    ```\n",
    "\n",
    "    - then we create the `docker-compose.yaml` file to include the two services and ports\n",
    "\n",
    "        ```yaml\n",
    "        version: \"3.9\"\n",
    "        services:\n",
    "        clothing-model:\n",
    "            image: zoomcamp-10-model:xception-v4-001\n",
    "        gateway:\n",
    "            image: zoomcamp-10-gateway:001\n",
    "            environment:\n",
    "            - TF_SERVING_HOST=clothing-model:8500\n",
    "            ports:\n",
    "            - \"9696:9696\"\n",
    "        ```\n",
    "\n",
    "    - run docker compose (in the same dir as yaml file)\n",
    "        ```\n",
    "        docker compose up -d\n",
    "        ```\n",
    "\n",
    "    and test using test.py\n",
    "\n",
    "    This way we only need to run it once instead of spinning up two containers as before.\n",
    "\n",
    "    - stop it using `docker compose down`\n",
    "\n",
    "This is how we use docker compose to run multiple connected services on the same machine!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubernetes\n",
    "\n",
    "* [The anatomy of a Kubernetes cluster](https://youtu.be/UjVkpszDzgk?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR)\n",
    "\n",
    "**Kubernetes (K8s)** - open-source system for automating deployment, scaling and management of containerized applications. What it means --- it gives us a way to take docker image that we created locally and deploy to the cloud!\n",
    "\n",
    "to be continued ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KServe (Chapter 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataScienceEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
